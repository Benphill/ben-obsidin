While artificial intelligence has been heralded as humanity's greatest technological breakthrough, we stand at a crossroads where the risks may fundamentally outweigh the benefits. The rapid, largely unregulated development of AI systems threatens to undermine the very foundations of human society. People have been blinded by potential benefits and have not stopped to think about the consequences of how AI is improving. We can't let ourselves be convinced of the promised future where AI saves everyone, because if we believe that, AI will have no problem getting the upper hand.

**AI in War**

**Autonomous weapons systems** could lower the threshold for armed conflict by reducing human costs for the deploying nation, potentially making leaders more willing to engage in hostilities. These systems also raise profound ethical questions about machines making life-and-death decisions without human oversight.

**Speed and escalation risks** emerge as AI-enabled systems can analyze threats and respond faster than human decision-makers can intervene. This compression of decision-making timelines could lead to rapid escalation of conflicts before diplomatic solutions can be pursued.

According to the United Nations, as AI systems are trained on incomplete or biased data, they may misidentify civilians as combatants or make flawed threat assessments, potentially leading to a dramatic increase in war crimes and civilian casualties.

**Proliferation concerns** arise because AI military technologies could spread to non-state actors, terrorist groups, or nations with poor human rights records, potentially destabilizing regional security and making conflicts more unpredictable.

**Cyber warfare amplification** becomes possible as AI can automate and scale cyberattacks, potentially targeting critical infrastructure like power grids, hospitals, or financial systems, blurring the lines between military and civilian targets.

**Strategic instability** may increase as nations race to develop AI military capabilities, potentially triggering arms races and creating pressure to deploy insufficiently tested systems. The perceived advantage of striking first with AI weapons could paradoxically make conflicts more likely.

**Accountability gaps** emerge when autonomous systems cause harm, making it difficult to assign responsibility and potentially undermining international humanitarian law and war crime prosecution.


In regard to your comment pre